{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c1fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nate 뉴스기사 제목 스크래핑하기\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def scrape_nate_news():\n",
    "    # 섹션 정보 (섹션명: URL)\n",
    "    sections = {\n",
    "        \"최신뉴스\": \"https://news.nate.com/recent?mid=n0100\",\n",
    "        \"정치\": \"https://news.nate.com/recent?mid=n0200\",\n",
    "        \"경제\": \"https://news.nate.com/recent?mid=n0300\",\n",
    "        \"사회\": \"https://news.nate.com/recent?mid=n0400\",\n",
    "        \"세계\": \"https://news.nate.com/recent?mid=n0500\",\n",
    "        \"IT/과학\": \"https://news.nate.com/recent?mid=n0600\"\n",
    "    }\n",
    "    \n",
    "    for section_name, section_url in sections.items():\n",
    "        print(f\"\\n=== {section_name} 섹션 ===\")\n",
    "        \n",
    "        try:\n",
    "            # 페이지 요청\n",
    "            headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "            response = requests.get(section_url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # HTML 파싱\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # 기사 목록 추출\n",
    "            articles = soup.select('.mduSubjectList .mlt01')\n",
    "            \n",
    "            for i, article in enumerate(articles[:5], 1):  # 각 섹션에서 상위 5개 기사만 출력\n",
    "                # 제목 추출\n",
    "                title_tag = article.select_one('h2.tit')\n",
    "                title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "                \n",
    "                # 링크 추출\n",
    "                link_tag = article.find('a')\n",
    "                link = urljoin(section_url, link_tag['href']) if link_tag and 'href' in link_tag.attrs else \"링크 없음\"\n",
    "                \n",
    "                # 이미지 추출\n",
    "                img_tag = article.select_one('img')\n",
    "                if img_tag and 'src' in img_tag.attrs:\n",
    "                    img_url = urljoin(section_url, img_tag['src'])\n",
    "                    # 이미지가 없는 경우를 대비한 onerror 속성 확인\n",
    "                    if 'onerror' in img_tag.attrs:\n",
    "                        # onerror 속성에서 실제 이미지 URL 추출\n",
    "                        onerror = img_tag['onerror']\n",
    "                        if 'ImgError' in onerror:\n",
    "                            # ImgError 함수에서 실제 이미지 URL 추출\n",
    "                            parts = onerror.split(',')\n",
    "                            if len(parts) >= 2:\n",
    "                                actual_url = parts[1].strip().strip(\"'\\\"\")\n",
    "                                img_url = urljoin(section_url, actual_url)\n",
    "                    \n",
    "                    try:\n",
    "                        # 이미지 출력\n",
    "                        display(Image(url=img_url, width=100))\n",
    "                        print(f\"이미지 URL: {img_url}\")\n",
    "                    except:\n",
    "                        print(\"이미지를 불러올 수 없습니다.\")\n",
    "                else:\n",
    "                    print(\"이미지 없음\")\n",
    "                \n",
    "                print(f\"{i}. 제목: {title}\")\n",
    "                print(f\"   링크: {link}\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"{section_name} 섹션을 가져오는 중 오류 발생: {e}\")\n",
    "\n",
    "# 함수 실행\n",
    "scrape_nate_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92887eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#하나의 네이버 웹툰과 1개의 회차에 대한 Image 다운로드 하기\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def download_one_episode(title, no, url):\n",
    "    # 이미지 저장 디렉토리 생성\n",
    "    dir_path = f\"img/{title}/{no}\"\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    # 웹툰 페이지 가져오기\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 이미지 태그 찾기\n",
    "    img_tags = soup.select('#comic_view_area img')\n",
    "    \n",
    "    # 각 이미지 다운로드\n",
    "    for idx, img in enumerate(img_tags):\n",
    "        img_url = img['src']\n",
    "        \n",
    "        # 이미지 다운로드\n",
    "        img_data = requests.get(img_url, headers=headers).content\n",
    "        \n",
    "        # 파일 저장 (이미지 확장자는 jpg로 가정)\n",
    "        file_path = f\"{dir_path}/{idx+1}.jpg\"\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(img_data)\n",
    "        \n",
    "        print(f\"Downloaded: {file_path}\")\n",
    "    \n",
    "    print(f\"All images downloaded to {dir_path}\")\n",
    "\n",
    "# 예시 사용법\n",
    "download_one_episode('잔불의 기사', 198, 'https://comic.naver.com/webtoon/detail?titleId=768536&no=198&week=mon')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
